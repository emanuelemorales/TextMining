{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT MINING PROJECT\n",
    "\n",
    "### Morales Emanuele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emamo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emamo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emamo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\emamo\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "from nltk import *\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from ftfy import fix_encoding\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: Text data pre-processing on Facebook comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataframe\n",
    "df = pd.read_csv(\"fb_sentiment.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select just the column of interest (comments)\n",
    "\n",
    "first_column = df.columns[0]\n",
    "second_column = df.columns[2]\n",
    "\n",
    "df = df.drop([first_column], axis=1)\n",
    "df = df.drop([second_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBPost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drug Runners and  a U.S. Senator have somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heres a single, to add, to Kindle. Just read t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I liked it.  Its youth oriented and I think th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think the point of the commercial is that, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Kindle 3 is such a great product. I could not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>develop a way to share books!  that is a big d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I love my kindle! =)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                FBPost\n",
       "0    Drug Runners and  a U.S. Senator have somethin...\n",
       "1    Heres a single, to add, to Kindle. Just read t...\n",
       "2    If you tire of Non-Fiction.. Check out http://...\n",
       "3      Ghost of Round Island is supposedly nonfiction.\n",
       "4    Why is Barnes and Nobles version of the Kindle...\n",
       "..                                                 ...\n",
       "995  I liked it.  Its youth oriented and I think th...\n",
       "996  I think the point of the commercial is that, e...\n",
       "997  Kindle 3 is such a great product. I could not ...\n",
       "998  develop a way to share books!  that is a big d...\n",
       "999                               I love my kindle! =)\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "### 1- Clean the corpus by eliminating punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation by using pandas and regex (regular expression):\n",
    "\n",
    "df[\"FBPost_nopunct\"] = df['FBPost'].str.replace('[^\\w\\s]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a list of english stopwords:\n",
    "\n",
    "stop_words_en = stopwords.words(\"english\") \n",
    "stop_words_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to dataframe without stopwords and punctuations and uppercase letters\n",
    "nrows = df.shape[0]\n",
    "stopwords_cleaned = []\n",
    "\n",
    "for x in range(nrows):\n",
    "    out1 = [w for w in nltk.word_tokenize(df['FBPost_nopunct'][x].lower()) if w not in stop_words_en]\n",
    "    out2 = ' '.join(out1).strip()\n",
    "    stopwords_cleaned.append(out2)\n",
    "    \n",
    "df[\"FB_nostopwords\"] = stopwords_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBPost</th>\n",
       "      <th>FBPost_nopunct</th>\n",
       "      <th>FB_nostopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drug Runners and  a U.S. Senator have somethin...</td>\n",
       "      <td>Drug Runners and  a U S  Senator have somethin...</td>\n",
       "      <td>drug runners u senator something murder http w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heres a single, to add, to Kindle. Just read t...</td>\n",
       "      <td>Heres a single  to add  to Kindle  Just read t...</td>\n",
       "      <td>heres single add kindle read 19th century stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
       "      <td>If you tire of Non Fiction   Check out http   ...</td>\n",
       "      <td>tire non fiction check http www amazon com ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction</td>\n",
       "      <td>ghost round island supposedly nonfiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>barnes nobles version kindle much expensive ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I liked it.  Its youth oriented and I think th...</td>\n",
       "      <td>I liked it   Its youth oriented and I think th...</td>\n",
       "      <td>liked youth oriented think widen appeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think the point of the commercial is that, e...</td>\n",
       "      <td>I think the point of the commercial is that  e...</td>\n",
       "      <td>think point commercial even borders closing ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Kindle 3 is such a great product. I could not ...</td>\n",
       "      <td>Kindle 3 is such a great product  I could not ...</td>\n",
       "      <td>kindle 3 great product could happier mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>develop a way to share books!  that is a big d...</td>\n",
       "      <td>develop a way to share books   that is a big d...</td>\n",
       "      <td>develop way share books big drawback love kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I love my kindle! =)</td>\n",
       "      <td>I love my kindle</td>\n",
       "      <td>love kindle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                FBPost  \\\n",
       "0    Drug Runners and  a U.S. Senator have somethin...   \n",
       "1    Heres a single, to add, to Kindle. Just read t...   \n",
       "2    If you tire of Non-Fiction.. Check out http://...   \n",
       "3      Ghost of Round Island is supposedly nonfiction.   \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it.  Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that, e...   \n",
       "997  Kindle 3 is such a great product. I could not ...   \n",
       "998  develop a way to share books!  that is a big d...   \n",
       "999                               I love my kindle! =)   \n",
       "\n",
       "                                        FBPost_nopunct  \\\n",
       "0    Drug Runners and  a U S  Senator have somethin...   \n",
       "1    Heres a single  to add  to Kindle  Just read t...   \n",
       "2    If you tire of Non Fiction   Check out http   ...   \n",
       "3      Ghost of Round Island is supposedly nonfiction    \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it   Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that  e...   \n",
       "997  Kindle 3 is such a great product  I could not ...   \n",
       "998  develop a way to share books   that is a big d...   \n",
       "999                               I love my kindle       \n",
       "\n",
       "                                        FB_nostopwords  \n",
       "0    drug runners u senator something murder http w...  \n",
       "1    heres single add kindle read 19th century stor...  \n",
       "2    tire non fiction check http www amazon com ref...  \n",
       "3             ghost round island supposedly nonfiction  \n",
       "4    barnes nobles version kindle much expensive ki...  \n",
       "..                                                 ...  \n",
       "995            liked youth oriented think widen appeal  \n",
       "996  think point commercial even borders closing ma...  \n",
       "997          kindle 3 great product could happier mine  \n",
       "998  develop way share books big drawback love kind...  \n",
       "999                                        love kindle  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of rows of dataframe\n",
    "nrows = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vector of list containig the tokenized comments and insert a column in dataset containing the comments tokenized.\n",
    "\n",
    "tok = []\n",
    "\n",
    "for x in range(nrows):\n",
    "    comment = nltk.word_tokenize(df[\"FB_nostopwords\"][x])\n",
    "    tok.append(comment) \n",
    "    \n",
    "df[\"tokenised\"] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBPost</th>\n",
       "      <th>FBPost_nopunct</th>\n",
       "      <th>FB_nostopwords</th>\n",
       "      <th>tokenised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drug Runners and  a U.S. Senator have somethin...</td>\n",
       "      <td>Drug Runners and  a U S  Senator have somethin...</td>\n",
       "      <td>drug runners u senator something murder http w...</td>\n",
       "      <td>[drug, runners, u, senator, something, murder,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heres a single, to add, to Kindle. Just read t...</td>\n",
       "      <td>Heres a single  to add  to Kindle  Just read t...</td>\n",
       "      <td>heres single add kindle read 19th century stor...</td>\n",
       "      <td>[heres, single, add, kindle, read, 19th, centu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
       "      <td>If you tire of Non Fiction   Check out http   ...</td>\n",
       "      <td>tire non fiction check http www amazon com ref...</td>\n",
       "      <td>[tire, non, fiction, check, http, www, amazon,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction</td>\n",
       "      <td>ghost round island supposedly nonfiction</td>\n",
       "      <td>[ghost, round, island, supposedly, nonfiction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>barnes nobles version kindle much expensive ki...</td>\n",
       "      <td>[barnes, nobles, version, kindle, much, expens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I liked it.  Its youth oriented and I think th...</td>\n",
       "      <td>I liked it   Its youth oriented and I think th...</td>\n",
       "      <td>liked youth oriented think widen appeal</td>\n",
       "      <td>[liked, youth, oriented, think, widen, appeal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think the point of the commercial is that, e...</td>\n",
       "      <td>I think the point of the commercial is that  e...</td>\n",
       "      <td>think point commercial even borders closing ma...</td>\n",
       "      <td>[think, point, commercial, even, borders, clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Kindle 3 is such a great product. I could not ...</td>\n",
       "      <td>Kindle 3 is such a great product  I could not ...</td>\n",
       "      <td>kindle 3 great product could happier mine</td>\n",
       "      <td>[kindle, 3, great, product, could, happier, mine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>develop a way to share books!  that is a big d...</td>\n",
       "      <td>develop a way to share books   that is a big d...</td>\n",
       "      <td>develop way share books big drawback love kind...</td>\n",
       "      <td>[develop, way, share, books, big, drawback, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I love my kindle! =)</td>\n",
       "      <td>I love my kindle</td>\n",
       "      <td>love kindle</td>\n",
       "      <td>[love, kindle]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                FBPost  \\\n",
       "0    Drug Runners and  a U.S. Senator have somethin...   \n",
       "1    Heres a single, to add, to Kindle. Just read t...   \n",
       "2    If you tire of Non-Fiction.. Check out http://...   \n",
       "3      Ghost of Round Island is supposedly nonfiction.   \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it.  Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that, e...   \n",
       "997  Kindle 3 is such a great product. I could not ...   \n",
       "998  develop a way to share books!  that is a big d...   \n",
       "999                               I love my kindle! =)   \n",
       "\n",
       "                                        FBPost_nopunct  \\\n",
       "0    Drug Runners and  a U S  Senator have somethin...   \n",
       "1    Heres a single  to add  to Kindle  Just read t...   \n",
       "2    If you tire of Non Fiction   Check out http   ...   \n",
       "3      Ghost of Round Island is supposedly nonfiction    \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it   Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that  e...   \n",
       "997  Kindle 3 is such a great product  I could not ...   \n",
       "998  develop a way to share books   that is a big d...   \n",
       "999                               I love my kindle       \n",
       "\n",
       "                                        FB_nostopwords  \\\n",
       "0    drug runners u senator something murder http w...   \n",
       "1    heres single add kindle read 19th century stor...   \n",
       "2    tire non fiction check http www amazon com ref...   \n",
       "3             ghost round island supposedly nonfiction   \n",
       "4    barnes nobles version kindle much expensive ki...   \n",
       "..                                                 ...   \n",
       "995            liked youth oriented think widen appeal   \n",
       "996  think point commercial even borders closing ma...   \n",
       "997          kindle 3 great product could happier mine   \n",
       "998  develop way share books big drawback love kind...   \n",
       "999                                        love kindle   \n",
       "\n",
       "                                             tokenised  \n",
       "0    [drug, runners, u, senator, something, murder,...  \n",
       "1    [heres, single, add, kindle, read, 19th, centu...  \n",
       "2    [tire, non, fiction, check, http, www, amazon,...  \n",
       "3       [ghost, round, island, supposedly, nonfiction]  \n",
       "4    [barnes, nobles, version, kindle, much, expens...  \n",
       "..                                                 ...  \n",
       "995     [liked, youth, oriented, think, widen, appeal]  \n",
       "996  [think, point, commercial, even, borders, clos...  \n",
       "997  [kindle, 3, great, product, could, happier, mine]  \n",
       "998  [develop, way, share, books, big, drawback, lo...  \n",
       "999                                     [love, kindle]  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Try to obtain bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It creates a sequence of two adjacents words for each of the comments\n",
    "\n",
    "bigrams_vec=[]\n",
    "\n",
    "for x in range(nrows):\n",
    "    bigrams = nltk.ngrams(df[\"tokenised\"][x], n = 2)\n",
    "    bigrams_vec.append(bigrams)\n",
    "\n",
    "df[\"bigrams\"] = bigrams_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBPost</th>\n",
       "      <th>FBPost_nopunct</th>\n",
       "      <th>FB_nostopwords</th>\n",
       "      <th>tokenised</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drug Runners and  a U.S. Senator have somethin...</td>\n",
       "      <td>Drug Runners and  a U S  Senator have somethin...</td>\n",
       "      <td>drug runners u senator something murder http w...</td>\n",
       "      <td>[drug, runners, u, senator, something, murder,...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DD9E0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heres a single, to add, to Kindle. Just read t...</td>\n",
       "      <td>Heres a single  to add  to Kindle  Just read t...</td>\n",
       "      <td>heres single add kindle read 19th century stor...</td>\n",
       "      <td>[heres, single, add, kindle, read, 19th, centu...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DDF20&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
       "      <td>If you tire of Non Fiction   Check out http   ...</td>\n",
       "      <td>tire non fiction check http www amazon com ref...</td>\n",
       "      <td>[tire, non, fiction, check, http, www, amazon,...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DDAC0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction</td>\n",
       "      <td>ghost round island supposedly nonfiction</td>\n",
       "      <td>[ghost, round, island, supposedly, nonfiction]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DDE40&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>barnes nobles version kindle much expensive ki...</td>\n",
       "      <td>[barnes, nobles, version, kindle, much, expens...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DD890&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I liked it.  Its youth oriented and I think th...</td>\n",
       "      <td>I liked it   Its youth oriented and I think th...</td>\n",
       "      <td>liked youth oriented think widen appeal</td>\n",
       "      <td>[liked, youth, oriented, think, widen, appeal]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E22E0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think the point of the commercial is that, e...</td>\n",
       "      <td>I think the point of the commercial is that  e...</td>\n",
       "      <td>think point commercial even borders closing ma...</td>\n",
       "      <td>[think, point, commercial, even, borders, clos...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E2350&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Kindle 3 is such a great product. I could not ...</td>\n",
       "      <td>Kindle 3 is such a great product  I could not ...</td>\n",
       "      <td>kindle 3 great product could happier mine</td>\n",
       "      <td>[kindle, 3, great, product, could, happier, mine]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E23C0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>develop a way to share books!  that is a big d...</td>\n",
       "      <td>develop a way to share books   that is a big d...</td>\n",
       "      <td>develop way share books big drawback love kind...</td>\n",
       "      <td>[develop, way, share, books, big, drawback, lo...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E2430&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I love my kindle! =)</td>\n",
       "      <td>I love my kindle</td>\n",
       "      <td>love kindle</td>\n",
       "      <td>[love, kindle]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E24A0&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                FBPost  \\\n",
       "0    Drug Runners and  a U.S. Senator have somethin...   \n",
       "1    Heres a single, to add, to Kindle. Just read t...   \n",
       "2    If you tire of Non-Fiction.. Check out http://...   \n",
       "3      Ghost of Round Island is supposedly nonfiction.   \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it.  Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that, e...   \n",
       "997  Kindle 3 is such a great product. I could not ...   \n",
       "998  develop a way to share books!  that is a big d...   \n",
       "999                               I love my kindle! =)   \n",
       "\n",
       "                                        FBPost_nopunct  \\\n",
       "0    Drug Runners and  a U S  Senator have somethin...   \n",
       "1    Heres a single  to add  to Kindle  Just read t...   \n",
       "2    If you tire of Non Fiction   Check out http   ...   \n",
       "3      Ghost of Round Island is supposedly nonfiction    \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it   Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that  e...   \n",
       "997  Kindle 3 is such a great product  I could not ...   \n",
       "998  develop a way to share books   that is a big d...   \n",
       "999                               I love my kindle       \n",
       "\n",
       "                                        FB_nostopwords  \\\n",
       "0    drug runners u senator something murder http w...   \n",
       "1    heres single add kindle read 19th century stor...   \n",
       "2    tire non fiction check http www amazon com ref...   \n",
       "3             ghost round island supposedly nonfiction   \n",
       "4    barnes nobles version kindle much expensive ki...   \n",
       "..                                                 ...   \n",
       "995            liked youth oriented think widen appeal   \n",
       "996  think point commercial even borders closing ma...   \n",
       "997          kindle 3 great product could happier mine   \n",
       "998  develop way share books big drawback love kind...   \n",
       "999                                        love kindle   \n",
       "\n",
       "                                             tokenised  \\\n",
       "0    [drug, runners, u, senator, something, murder,...   \n",
       "1    [heres, single, add, kindle, read, 19th, centu...   \n",
       "2    [tire, non, fiction, check, http, www, amazon,...   \n",
       "3       [ghost, round, island, supposedly, nonfiction]   \n",
       "4    [barnes, nobles, version, kindle, much, expens...   \n",
       "..                                                 ...   \n",
       "995     [liked, youth, oriented, think, widen, appeal]   \n",
       "996  [think, point, commercial, even, borders, clos...   \n",
       "997  [kindle, 3, great, product, could, happier, mine]   \n",
       "998  [develop, way, share, books, big, drawback, lo...   \n",
       "999                                     [love, kindle]   \n",
       "\n",
       "                                             bigrams  \n",
       "0    <generator object ngrams at 0x000001B6E13DD9E0>  \n",
       "1    <generator object ngrams at 0x000001B6E13DDF20>  \n",
       "2    <generator object ngrams at 0x000001B6E13DDAC0>  \n",
       "3    <generator object ngrams at 0x000001B6E13DDE40>  \n",
       "4    <generator object ngrams at 0x000001B6E13DD890>  \n",
       "..                                               ...  \n",
       "995  <generator object ngrams at 0x000001B6E14E22E0>  \n",
       "996  <generator object ngrams at 0x000001B6E14E2350>  \n",
       "997  <generator object ngrams at 0x000001B6E14E23C0>  \n",
       "998  <generator object ngrams at 0x000001B6E14E2430>  \n",
       "999  <generator object ngrams at 0x000001B6E14E24A0>  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tire', 'non')\n",
      "('non', 'fiction')\n",
      "('fiction', 'check')\n",
      "('check', 'http')\n",
      "('http', 'www')\n",
      "('www', 'amazon')\n",
      "('amazon', 'com')\n",
      "('com', 'ref')\n",
      "('ref', 'nb_sb_noss')\n",
      "('nb_sb_noss', 'url')\n",
      "('url', 'search')\n",
      "('search', 'alias')\n",
      "('alias', '3daps')\n",
      "('3daps', 'field')\n",
      "('field', 'keywords')\n",
      "('keywords', 'danielle')\n",
      "('danielle', 'lee')\n",
      "('lee', 'zwissler')\n",
      "('zwissler', 'x')\n",
      "('x', '0')\n",
      "('0', '0')\n"
     ]
    }
   ],
   "source": [
    "#Access to a specific Bigram in the dataset\n",
    "\n",
    "for bigram in df[\"bigrams\"][2]:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: \n",
    "### 1- Split the original corpus in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It creates a vector of lists containing the sentences composing each comments \n",
    "\n",
    "sentences_vec=[]\n",
    "\n",
    "for x in range(nrows):\n",
    "    sent = nltk.sent_tokenize(df[\"FBPost\"][x])\n",
    "    sentences_vec.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences\"] = sentences_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBPost</th>\n",
       "      <th>FBPost_nopunct</th>\n",
       "      <th>FB_nostopwords</th>\n",
       "      <th>tokenised</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drug Runners and  a U.S. Senator have somethin...</td>\n",
       "      <td>Drug Runners and  a U S  Senator have somethin...</td>\n",
       "      <td>drug runners u senator something murder http w...</td>\n",
       "      <td>[drug, runners, u, senator, something, murder,...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DD9E0&gt;</td>\n",
       "      <td>[Drug Runners and  a U.S., Senator have someth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heres a single, to add, to Kindle. Just read t...</td>\n",
       "      <td>Heres a single  to add  to Kindle  Just read t...</td>\n",
       "      <td>heres single add kindle read 19th century stor...</td>\n",
       "      <td>[heres, single, add, kindle, read, 19th, centu...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DDF20&gt;</td>\n",
       "      <td>[Heres a single, to add, to Kindle., Just read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
       "      <td>If you tire of Non Fiction   Check out http   ...</td>\n",
       "      <td>tire non fiction check http www amazon com ref...</td>\n",
       "      <td>[tire, non, fiction, check, http, www, amazon,...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DDAC0&gt;</td>\n",
       "      <td>[If you tire of Non-Fiction.., Check out http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction</td>\n",
       "      <td>ghost round island supposedly nonfiction</td>\n",
       "      <td>[ghost, round, island, supposedly, nonfiction]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DDE40&gt;</td>\n",
       "      <td>[Ghost of Round Island is supposedly nonfiction.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>barnes nobles version kindle much expensive ki...</td>\n",
       "      <td>[barnes, nobles, version, kindle, much, expens...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E13DD890&gt;</td>\n",
       "      <td>[Why is Barnes and Nobles version of the Kindl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I liked it.  Its youth oriented and I think th...</td>\n",
       "      <td>I liked it   Its youth oriented and I think th...</td>\n",
       "      <td>liked youth oriented think widen appeal</td>\n",
       "      <td>[liked, youth, oriented, think, widen, appeal]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E22E0&gt;</td>\n",
       "      <td>[I liked it., Its youth oriented and I think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think the point of the commercial is that, e...</td>\n",
       "      <td>I think the point of the commercial is that  e...</td>\n",
       "      <td>think point commercial even borders closing ma...</td>\n",
       "      <td>[think, point, commercial, even, borders, clos...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E2350&gt;</td>\n",
       "      <td>[I think the point of the commercial is that, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Kindle 3 is such a great product. I could not ...</td>\n",
       "      <td>Kindle 3 is such a great product  I could not ...</td>\n",
       "      <td>kindle 3 great product could happier mine</td>\n",
       "      <td>[kindle, 3, great, product, could, happier, mine]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E23C0&gt;</td>\n",
       "      <td>[Kindle 3 is such a great product., I could no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>develop a way to share books!  that is a big d...</td>\n",
       "      <td>develop a way to share books   that is a big d...</td>\n",
       "      <td>develop way share books big drawback love kind...</td>\n",
       "      <td>[develop, way, share, books, big, drawback, lo...</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E2430&gt;</td>\n",
       "      <td>[develop a way to share books!, that is a big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I love my kindle! =)</td>\n",
       "      <td>I love my kindle</td>\n",
       "      <td>love kindle</td>\n",
       "      <td>[love, kindle]</td>\n",
       "      <td>&lt;generator object ngrams at 0x000001B6E14E24A0&gt;</td>\n",
       "      <td>[I love my kindle!, =)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                FBPost  \\\n",
       "0    Drug Runners and  a U.S. Senator have somethin...   \n",
       "1    Heres a single, to add, to Kindle. Just read t...   \n",
       "2    If you tire of Non-Fiction.. Check out http://...   \n",
       "3      Ghost of Round Island is supposedly nonfiction.   \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it.  Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that, e...   \n",
       "997  Kindle 3 is such a great product. I could not ...   \n",
       "998  develop a way to share books!  that is a big d...   \n",
       "999                               I love my kindle! =)   \n",
       "\n",
       "                                        FBPost_nopunct  \\\n",
       "0    Drug Runners and  a U S  Senator have somethin...   \n",
       "1    Heres a single  to add  to Kindle  Just read t...   \n",
       "2    If you tire of Non Fiction   Check out http   ...   \n",
       "3      Ghost of Round Island is supposedly nonfiction    \n",
       "4    Why is Barnes and Nobles version of the Kindle...   \n",
       "..                                                 ...   \n",
       "995  I liked it   Its youth oriented and I think th...   \n",
       "996  I think the point of the commercial is that  e...   \n",
       "997  Kindle 3 is such a great product  I could not ...   \n",
       "998  develop a way to share books   that is a big d...   \n",
       "999                               I love my kindle       \n",
       "\n",
       "                                        FB_nostopwords  \\\n",
       "0    drug runners u senator something murder http w...   \n",
       "1    heres single add kindle read 19th century stor...   \n",
       "2    tire non fiction check http www amazon com ref...   \n",
       "3             ghost round island supposedly nonfiction   \n",
       "4    barnes nobles version kindle much expensive ki...   \n",
       "..                                                 ...   \n",
       "995            liked youth oriented think widen appeal   \n",
       "996  think point commercial even borders closing ma...   \n",
       "997          kindle 3 great product could happier mine   \n",
       "998  develop way share books big drawback love kind...   \n",
       "999                                        love kindle   \n",
       "\n",
       "                                             tokenised  \\\n",
       "0    [drug, runners, u, senator, something, murder,...   \n",
       "1    [heres, single, add, kindle, read, 19th, centu...   \n",
       "2    [tire, non, fiction, check, http, www, amazon,...   \n",
       "3       [ghost, round, island, supposedly, nonfiction]   \n",
       "4    [barnes, nobles, version, kindle, much, expens...   \n",
       "..                                                 ...   \n",
       "995     [liked, youth, oriented, think, widen, appeal]   \n",
       "996  [think, point, commercial, even, borders, clos...   \n",
       "997  [kindle, 3, great, product, could, happier, mine]   \n",
       "998  [develop, way, share, books, big, drawback, lo...   \n",
       "999                                     [love, kindle]   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0    <generator object ngrams at 0x000001B6E13DD9E0>   \n",
       "1    <generator object ngrams at 0x000001B6E13DDF20>   \n",
       "2    <generator object ngrams at 0x000001B6E13DDAC0>   \n",
       "3    <generator object ngrams at 0x000001B6E13DDE40>   \n",
       "4    <generator object ngrams at 0x000001B6E13DD890>   \n",
       "..                                               ...   \n",
       "995  <generator object ngrams at 0x000001B6E14E22E0>   \n",
       "996  <generator object ngrams at 0x000001B6E14E2350>   \n",
       "997  <generator object ngrams at 0x000001B6E14E23C0>   \n",
       "998  <generator object ngrams at 0x000001B6E14E2430>   \n",
       "999  <generator object ngrams at 0x000001B6E14E24A0>   \n",
       "\n",
       "                                             sentences  \n",
       "0    [Drug Runners and  a U.S., Senator have someth...  \n",
       "1    [Heres a single, to add, to Kindle., Just read...  \n",
       "2    [If you tire of Non-Fiction.., Check out http:...  \n",
       "3    [Ghost of Round Island is supposedly nonfiction.]  \n",
       "4    [Why is Barnes and Nobles version of the Kindl...  \n",
       "..                                                 ...  \n",
       "995  [I liked it., Its youth oriented and I think t...  \n",
       "996  [I think the point of the commercial is that, ...  \n",
       "997  [Kindle 3 is such a great product., I could no...  \n",
       "998  [develop a way to share books!, that is a big ...  \n",
       "999                            [I love my kindle!, =)]  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The corpus is composed by the comments without punctuation, stopwords and uppercase letters\n",
    "corpus = df[\"FB_nostopwords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '10', '100', '10oz', '11', '12', '1295910892', '1298310486', '14', '18', '19', '1900', '19th', '1st', '20', '200', '2010', '2011', '2012', '20th', '23', '24', '25', '2nd', '30', '321', '3500', '39', '3daps', '3g', '41', '49', '4th', '500', '5th', '5us', '60', '75', '79', '866', '87', '8851', '90', '94', '99', '99cents', 'aa', 'ability', 'able', 'abroad', 'absolute', 'absolutely', 'access', 'accessories', 'accident', 'accidentally', 'accidently', 'accomodate', 'accompanying', 'account', 'ache', 'across', 'acting', 'activities', 'actual', 'actually', 'ad', 'add', 'addicted', 'addicting', 'addiction', 'addictive', 'adjust', 'adjustments', 'adopt', 'adore', 'adoro', 'adquirido', 'adult', 'adults', 'adventure', 'advert', 'advertise', 'advertised', 'advertisement', 'advertising', 'advise', 'aesop', 'afair', 'affair', 'afford', 'afordable', 'afraid', 'africa', 'afterwards', 'ag56twvu5xwc2', 'agenda', 'ages', 'aggravating', 'ago', 'agree', 'agreed', 'airplane', 'al', 'albeit', 'alerts', 'alias', 'alice', 'alike', 'alladin', 'alleviate', 'allow', 'allowed', 'almost', 'aloha', 'alone', 'along', 'alphabetical', 'alphabetize', 'already', 'also', 'alternative', 'although', 'always', 'amazing', 'amazon', 'amazons', 'amen', 'america', 'american', 'amigos', 'among', 'amount', 'anders', 'andi', 'andmuch', 'andrea', 'android', 'andy', 'angel', 'angle', 'angry', 'ann', 'anna', 'annette', 'annoying', 'another', 'answer', 'anticipated', 'antiquing', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anywhere', 'ap', 'apart', 'apologetic', 'app', 'appeal', 'appear', 'appears', 'appointments', 'appreciate', 'appreciated', 'apps', 'appt', 'archaic', 'archive', 'archived', 'archives', 'archiving', 'arcives', 'areas', 'arent', 'arm', 'armrest', 'arms', 'around', 'arrived', 'arthritis', 'article', 'articles', 'artwork', 'ask', 'asked', 'asks', 'assume', 'assuming', 'attach', 'attached', 'attitude', 'attorney', 'audio', 'author', 'authors', 'autobiography', 'automatically', 'available', 'avenue', 'average', 'avid', 'avoiding', 'awake', 'away', 'awesome', 'awful', 'awhile', 'awkward', 'aww', 'aÃ±o', 'b003es5nys', 'b0045jnbnu', 'b004ayd6lw', 'b004fpz452', 'babcockpublishing', 'babies', 'baby', 'back', 'backlight', 'backpack', 'backpacks', 'backup', 'bad', 'badly', 'bag', 'bags', 'bajas', 'balancing', 'band', 'bandwagon', 'bangs', 'bank', 'barbara', 'barley', 'barnes', 'based', 'basic', 'basics', 'batteries', 'battery', 'bc', 'beach', 'bearable', 'beat', 'beautiful', 'bed', 'bedroom', 'beginning', 'believe', 'belkin', 'beloved', 'benefit', 'best', 'bet', 'betsy', 'better', 'beyond', 'bible', 'bibles', 'bicycle', 'big', 'biggest', 'bike', 'bin', 'binder', 'binding', 'birthday', 'bit', 'bks', 'black', 'blackberry', 'blackjack', 'blaine', 'blanket', 'blessing', 'blood', 'bloody', 'blue', 'boarder', 'boarding', 'body', 'bolton', 'bomb', 'bone', 'bonus', 'boo', 'book', 'bookmark', 'bookmarked', 'bookmarks', 'books', 'bookstore', 'bookstores', 'boonies', 'bordering', 'borders', 'bored', 'boring', 'born', 'borrow', 'borrowed', 'bothering', 'bottom', 'bought', 'box', 'bozarth', 'bradley', 'brains', 'brand', 'bread', 'break', 'breastfeeding', 'breeding', 'bri', 'bright', 'brightest', 'brilliant', 'bring', 'brings', 'brochure', 'brought', 'brown', 'browse', 'btw', 'bubble', 'bucks', 'built', 'bulk', 'bulky', 'bummed', 'bummer', 'bumps', 'bunch', 'bundle', 'burn', 'bury', 'buses', 'businesses', 'busy', 'butter', 'button', 'buttons', 'buy', 'buying', 'bw', 'cabin', 'cable', 'caliber', 'calibre', 'call', 'called', 'calling', 'calm', 'camaflage', 'came', 'camo', 'canada', 'cant', 'capabilities', 'capitalized', 'caps', 'capturing', 'car', 'cara', 'carbon', 'card', 'careful', 'cares', 'carrier', 'carry', 'carrying', 'case', 'cases', 'cash', 'cat', 'cataracts', 'catch', 'catchy', 'categories', 'categorize', 'cats', 'caught', 'cause', 'causes', 'causing', 'caveats', 'cell', 'cents', 'century', 'certain', 'certainly', 'cetatinly', 'chair', 'challenge', 'chance', 'change', 'changed', 'changing', 'chapter', 'chapters', 'charge', 'charged', 'charger', 'charles', 'cheaper', 'cheboygan', 'check', 'checked', 'cheek', 'child', 'children', 'childs', 'chockablock', 'choice', 'choices', 'choose', 'chose', 'christmas', 'circumstantial', 'cities', 'citites', 'citizen', 'clancy', 'clara', 'class', 'classic', 'classics', 'classroom', 'clean', 'clear', 'clearer', 'clever', 'click', 'clicking', 'clip', 'clog', 'close', 'closed', 'closing', 'clotildes', 'clown', 'club', 'clue', 'clutter', 'coffee', 'coke', 'collected', 'collection', 'collections', 'collectionuse', 'college', 'color', 'colorful', 'colorless', 'com', 'combsif', 'comdoms', 'come', 'comes', 'coming', 'commend', 'comment', 'comments', 'commercial', 'commercials', 'communicate', 'community', 'commute', 'companion', 'company', 'compared', 'comparing', 'compatible', 'compete', 'competition', 'complain', 'complaining', 'complaint', 'completely', 'complicated', 'compose', 'comprÃ¡rselos', 'compu', 'computer', 'con', 'concerns', 'condoms', 'conduct', 'conectividad', 'conferences', 'confident', 'confino', 'confuse', 'confused', 'connect', 'cons', 'consider', 'console', 'constant', 'constantly', 'consuming', 'contact', 'contain', 'contained', 'contains', 'contemporary', 'content', 'continue', 'contraception', 'contractually', 'contrast', 'control', 'controls', 'convenient', 'convenit', 'conver', 'conversion', 'convert', 'converted', 'converting', 'convinced', 'convirtiÃ©ndolos', 'cook', 'cool', 'coolest', 'cop', 'copies', 'copy', 'copyrights', 'corner', 'cornered', 'cornwell', 'correct', 'correction', 'cost', 'could', 'couldnt', 'count', 'counting', 'countries', 'country', 'county', 'couple', 'course', 'cover', 'coverage', 'covered', 'covers', 'cozy', 'crackle', 'cracks', 'cradle', 'crap', 'create', 'credit', 'creepy', 'crime', 'cringed', 'crinkle', 'cristo', 'crochete', 'crowded', 'cruise', 'crumbs', 'cup', 'current', 'currently', 'curse', 'cursor', 'customer', 'customers', 'cute', 'cuter', 'cutrones', 'cutting', 'cyr', 'daily', 'dam', 'damage', 'dancing', 'dang', 'dangerous', 'dangerously', 'danielle', 'dannielle', 'dare', 'dark', 'darker', 'darn', 'darwin', 'date', 'daughter', 'daughters', 'daugjhter', 'dave', 'david', 'day', 'days', 'de', 'dead', 'deaf', 'deal', 'dean', 'dearly', 'debbie', 'decent', 'deceptive', 'decided', 'deciding', 'declaration', 'decorate', 'def', 'default', 'defeats', 'defected', 'defile', 'definately', 'define', 'defined', 'definitely', 'definition', 'definitions', 'defreeze', 'delete', 'deleted', 'deletes', 'deleting', 'delightful', 'dentists', 'dentro', 'depended', 'depending', 'depends', 'deregistered', 'deserves', 'desktop', 'desperate', 'despite', 'detailed', 'detriment', 'develop', 'developed', 'developing', 'device', 'devices', 'dexterous', 'dialog', 'diane', 'dickens', 'diction', 'dictionaries', 'dictionary', 'dictionnary', 'didnt', 'die', 'diego', 'difference', 'different', 'difficult', 'dig', 'digitales', 'dimensions', 'dining', 'dint', 'diplomas', 'direct', 'directional', 'directions', 'directly', 'director', 'dirt', 'disappear', 'disappeared', 'disappears', 'disappointing', 'disappoints', 'disc', 'disclose', 'discomfort', 'discovering', 'discussing', 'discussion', 'disgusting', 'display', 'displayed', 'dissertation', 'distance', 'distorted', 'dl', 'doable', 'doc', 'docs', 'doctor', 'doctors', 'document', 'documents', 'doesnt', 'dog', 'dollars', 'dominate', 'donate', 'donated', 'donating', 'done', 'dont', 'door', 'dora', 'dorks', 'dots', 'douglas', 'download', 'downloaded', 'downloading', 'downloads', 'downright', 'doyle', 'dozen', 'dozens', 'dp', 'dr', 'dragana', 'drained', 'drawback', 'dream', 'dress', 'dresser', 'drinking', 'drive', 'droid', 'drop', 'dropped', 'drug', 'dry', 'dude', 'due', 'duh', 'dumb', 'dust', 'dvds', 'dx', 'dying', 'dyslexia', 'dyslexics', 'e_inc', 'earned', 'earth', 'ease', 'easier', 'easily', 'easy', 'ebay', 'ebook', 'ebooks', 'echo', 'ecstatic', 'edge', 'edition', 'education', 'effects', 'effing', 'effort', 'effortless', 'effortlessly', 'eg', 'eh', 'either', 'el', 'elastic', 'electricity', 'electronic', 'electronics', 'electrÃ³nico', 'elegant', 'elephants', 'elliptical', 'else', 'elsewhere', 'em', 'email', 'emails', 'embarrassed', 'embarrassing', 'en', 'enabled', 'encourage', 'end', 'english', 'enhancements', 'enjoy', 'enjoyed', 'enjoying', 'enlarging', 'enough', 'enoyed', 'enter', 'entertainment', 'entire', 'entirely', 'entry', 'envelope', 'epub', 'ereader', 'ereaders', 'eric', 'erica', 'error', 'errors', 'es', 'esoteric', 'especially', 'essay', 'essence', 'essential', 'este', 'esto', 'etc', 'evans', 'even', 'evening', 'eventually', 'ever', 'everrrrrr', 'every', 'everyday', 'everyone', 'everything', 'everyting', 'everywhere', 'evidence', 'evo', 'exactly', 'examiner', 'example', 'excellent', 'excels', 'except', 'exchange', 'excited', 'excitement', 'excuse', 'exercise', 'exist', 'existence', 'exorcise', 'expandable', 'expanding', 'expecting', 'expensive', 'experience', 'experienced', 'explain', 'explained', 'explanation', 'extra', 'eye', 'eyes', 'eyestrain', 'eyre', 'ezbuy', 'fables', 'fabric', 'face', 'facebook', 'facility', 'fact', 'factor', 'fail', 'fair', 'fall', 'families', 'family', 'fan', 'fancy', 'fang', 'fantastic', 'far', 'farheen', 'fascinated', 'fashioned', 'fast', 'faster', 'fat', 'fault', 'fav', 'favor', 'favorite', 'favourite', 'favourites', 'fb', 'feature', 'features', 'feed', 'feedings', 'feel', 'fell', 'fellow', 'felt', 'feminine', 'fences', 'fetish', 'fi', 'fibromyalgia', 'fic', 'fiction', 'fiddling', 'field', 'fifth', 'fighting', 'figure', 'figured', 'file', 'files', 'finally', 'find', 'finding', 'fine', 'finesse', 'finger', 'fingure', 'finish', 'finished', 'first', 'fish', 'fished', 'fist', 'fit', 'fits', 'fitzwater', 'five', 'fix', 'flame', 'flap', 'flaps', 'flash', 'flippin', 'floated', 'floaters', 'floored', 'flop', 'florida', 'flower', 'fly', 'flying', 'fml', 'foam', 'folder', 'folders', 'folks', 'folletts', 'follow', 'fond', 'font', 'fonts', 'food', 'footnote', 'forbid', 'forced', 'forces', 'foreign', 'forests', 'forever', 'forget', 'forgetting', 'forgot', 'form', 'format', 'formats', 'forming', 'forth', 'fortunate', 'forum', 'forward', 'found', 'founders', 'frame', 'frances', 'free', 'freebies', 'freebooks', 'freeze', 'freezing', 'french', 'frequently', 'friday', 'fridays', 'friend', 'friendly', 'friends', 'frist', 'front', 'froze', 'full', 'fully', 'fun', 'function', 'functionally', 'fund', 'funky', 'funny', 'fussy', 'future', 'gadget', 'gag', 'gaines', 'galaxy', 'gale', 'game', 'games', 'gaming', 'gap', 'garbage', 'gave', 'gazillion', 'gear', 'gebre', 'geives', 'gen', 'generation', 'generator', 'genres', 'gently', 'georgette', 'german', 'get', 'gets', 'getting', 'ghettos', 'ghost', 'giant', 'gift', 'gifts', 'gim', 'ginger', 'girl', 'girls', 'give', 'given', 'gives', 'giving', 'gizmo', 'glad', 'glare', 'glasses', 'glitchy', 'globe', 'glory', 'go', 'goa', 'god', 'goes', 'going', 'gon', 'gone', 'good', 'google', 'got', 'gotten', 'govt', 'gp', 'grab', 'grade', 'graduates', 'graffiti', 'grandaughter', 'granddaughter', 'granddaughters', 'grandson', 'grasses', 'gratification', 'gratis', 'gray', 'great', 'greater', 'greatest', 'green', 'grey', 'grow', 'grr', 'grrrr', 'gt', 'guess', 'guessing', 'guide', 'gutenberg', 'guy', 'guys', 'gym', 'haha', 'hair', 'half', 'hand', 'handed', 'handle', 'hands', 'handy', 'hang', 'hangmans', 'happen', 'happended', 'happened', 'happening', 'happens', 'happier', 'happy', 'hard', 'hardback', 'hardcopy', 'harder', 'hardly', 'hardware', 'hasan', 'hassle', 'hate', 'havent', 'haves', 'hayley', 'head', 'headache', 'hear', 'heard', 'hears', 'heart', 'hearts', 'heavy', 'heck', 'hed', 'hehe', 'hehehehe', 'held', 'helen', 'hell', 'help', 'helped', 'helpful', 'helpfullness', 'helps', 'heres', 'herramienta', 'hes', 'hey', 'heyer', 'hi', 'high', 'higher', 'highlight', 'highlighted', 'highlights', 'highschool', 'hilite', 'hint', 'hints', 'historical', 'hit', 'hitting', 'hmm', 'hmmm', 'hold', 'holder', 'holding', 'holds', 'hole', 'holidays', 'home', 'homes', 'homeschool', 'homeschooling', 'honest', 'honestly', 'honey', 'hoo', 'hooray', 'hope', 'hopefully', 'hoping', 'hopped', 'horrible', 'horse', 'horses', 'hotspot', 'hour', 'hourly', 'hours', 'house', 'however', 'htm', 'http', 'hubby', 'huge', 'huh', 'hunger', 'hurdles', 'hurt', 'husband', 'husbands', 'hyphenations', 'ice', 'icon', 'id', 'idea', 'ideas', 'idiotic', 'idiots', 'ie', 'ignorant', 'ignored', 'ii', 'iin', 'ill', 'illiterate', 'illumitation', 'im', 'imagine', 'immediately', 'impatiently', 'important', 'impossible', 'impress', 'improvement', 'incapable', 'including', 'income', 'inconvenient', 'increase', 'index', 'india', 'indian', 'indie', 'individual', 'indulging', 'industry', 'inexpensive', 'infant', 'info', 'ing', 'ink', 'inpatient', 'inside', 'insisting', 'insists', 'installed', 'instance', 'instant', 'instead', 'instinctively', 'instruction', 'instructions', 'integration', 'intent', 'interactive', 'interest', 'interested', 'interesting', 'interface', 'internal', 'internet', 'intimidated', 'intrigued', 'introduced', 'invented', 'invention', 'investment', 'investments', 'involved', 'ipad', 'iphone', 'iphone4', 'ipod', 'ironic', 'irresponsible', 'island', 'isnt', 'issue', 'issues', 'italian', 'italy', 'itd', 'item', 'items', 'ive', 'jacek', 'jackass', 'jajajaja', 'jane', 'janice', 'jealous', 'jeffs', 'jennifer', 'jh', 'jingle', 'jj', 'jobs', 'jodi', 'joe', 'jogging', 'john', 'johnny', 'jokes', 'joseph', 'joy', 'joyce', 'joyful', 'joys', 'judy', 'juegos', 'juguete', 'julie', 'july', 'jump', 'jumps', 'junk', 'k2', 'k3', 'kaput', 'karen', 'karenina', 'kat', 'kathy', 'katie', 'keep', 'keeper', 'keeping', 'keeps', 'kellie', 'kelly', 'ken', 'kept', 'keurig', 'key', 'keywords', 'kid', 'kidding', 'kids', 'killer', 'killers', 'killing', 'kind', 'kinda', 'kindel', 'kindle', 'kindle2', 'kindle3', 'kindlealso', 'kindles', 'kindlexxx', 'kindly', 'kings', 'kitchen', 'kitty', 'kl', 'knew', 'kno', 'knocked', 'know', 'knowing', 'known', 'knows', 'kolan', 'konraths', 'koontz', 'lack', 'lacking', 'lacks', 'lalala', 'lamet', 'lamplight', 'language', 'languages', 'lap', 'laptop', 'large', 'larger', 'last', 'lasted', 'lasts', 'late', 'lately', 'later', 'latest', 'laughed', 'lay', 'laying', 'lazy', 'lb', 'leaping', 'learn', 'learned', 'learning', 'least', 'leather', 'leave', 'leaves', 'leaving', 'led', 'lee', 'leer', 'left', 'legnth', 'legthen', 'lend', 'lendable', 'lending', 'length', 'leslie', 'less', 'lesson', 'let', 'lets', 'letters', 'letting', 'level', 'levels', 'libraries', 'library', 'librarys', 'librays', 'libro', 'libros', 'life', 'lifestyle', 'light', 'lighted', 'lighting', 'lightweight', 'like', 'liked', 'likes', 'lime', 'limit', 'limited', 'lincoln', 'linda', 'line', 'link', 'linked', 'links', 'list', 'listed', 'listen', 'lists', 'lit', 'literally', 'little', 'live', 'lives', 'living', 'lo', 'load', 'loan', 'loaned', 'loaning', 'lobby', 'local', 'locate', 'located', 'location', 'lois', 'lol', 'lolbut', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lookup', 'loose', 'loosing', 'lori', 'los', 'losing', 'loss', 'lost', 'lot', 'lots', 'loud', 'louise', 'love', 'loved', 'lover', 'loves', 'loving', 'lovingly', 'low', 'lower', 'luck', 'lugging', 'luv', 'luvvvvvv', 'machine', 'mackinac', 'madam', 'made', 'magazines', 'magically', 'magnets', 'mail', 'main', 'mainers', 'major', 'make', 'maker', 'makes', 'making', 'makle', 'man', 'manage', 'managed', 'management', 'managing', 'manual', 'manually', 'many', 'margaret', 'margie', 'margin', 'maria', 'marion', 'market', 'marry', 'mas', 'match', 'material', 'matter', 'mature', 'may', 'maybe', 'mcclay', 'mean', 'meaning', 'meanings', 'means', 'meant', 'meanwhile', 'media', 'medical', 'medication', 'medium', 'meh', 'meisch', 'mejor', 'melodie', 'melody', 'memory', 'men', 'mention', 'mentioned', 'menu', 'meow', 'messages', 'messed', 'messes', 'meter', 'method', 'mexico', 'mi', 'miami', 'michelle', 'microphone', 'middle', 'might', 'mild', 'miles', 'million', 'min', 'mind', 'mine', 'mines', 'minimize', 'minor', 'minute', 'minutes', 'mis', 'misleading', 'miss', 'missed', 'missing', 'missingspaces', 'mmm', 'mne', 'mobi', 'moble', 'mode', 'model', 'modernizing', 'mom', 'moment', 'mommyhood', 'moms', 'monday', 'money', 'monitoring', 'monte', 'month', 'months', 'mood', 'moran', 'morning', 'mos', 'mostly', 'moth', 'mothership', 'motorhome', 'move', 'moved', 'moving', 'mr', 'much', 'multiple', 'mum', 'mundo', 'murder', 'music', 'must', 'muy', 'mysteries', 'mystery', 'na', 'naked', 'name', 'named', 'names', 'nap', 'nassau', 'native', 'navigating', 'navigations', 'nb_sb_noss', 'near', 'nearly', 'neat', 'necessary', 'need', 'needed', 'needs', 'negate', 'negative', 'netflicks', 'netflix', 'never', 'new', 'newer', 'newest', 'news', 'next', 'nice', 'nieces', 'niggle', 'night', 'nightly', 'nights', 'nobles', 'noise', 'non', 'nonexistent', 'nonfiction', 'nook', 'nooks', 'noooo', 'nope', 'normal', 'note', 'noted', 'notes', 'nothing', 'notice', 'noticed', 'novel', 'novels', 'novice', 'nuevo', 'number', 'numbers', 'nuprene', 'nursing', 'ny', 'nylon', 'obese', 'occasional', 'oct', 'oct10', 'october', 'oen', 'offense', 'offer', 'offering', 'offerings', 'offers', 'office', 'offices', 'official', 'often', 'oh', 'ok', 'old', 'older', 'oldest', 'omg', 'omission', 'one', 'ones', 'online', 'onto', 'oooh', 'ooze', 'open', 'opened', 'opening', 'opens', 'operating', 'ophthalmologists', 'opinion', 'option', 'options', 'order', 'ordered', 'orders', 'oriented', 'original', 'ortho', 'os', 'osteoarthritis', 'others', 'otherwise', 'outa', 'outage', 'outdoes', 'outside', 'overall', 'overdrive', 'overly', 'oversight', 'ow', 'owned', 'owner', 'owners', 'owning', 'oxford', 'pack', 'package', 'pad', 'padded', 'padding', 'page', 'pages', 'paid', 'pain', 'painful', 'pair', 'palooza', 'pandigital', 'panera', 'paper', 'paperback', 'paperbacks', 'papers', 'parka', 'parks', 'part', 'partly', 'pass', 'passage', 'passes', 'past', 'pastime', 'patience', 'patients', 'patricia', 'patterns', 'patterson', 'pay', 'paying', 'payroll', 'pc', 'pd_rhf_p_t_1', 'pdf', 'pdfs', 'peanut', 'pentz', 'people', 'peoples', 'per', 'percentage', 'perfect', 'perfectly', 'perfers', 'perhaps', 'period', 'periods', 'perm', 'permanently', 'perpetuate', 'person', 'personal', 'personally', 'persuade', 'peter', 'petting', 'pg', 'pharmacy', 'phone', 'phones', 'photos', 'phrase', 'phrases', 'phyllis', 'physical', 'pick', 'picked', 'picking', 'picoult', 'picoults', 'pics', 'picture', 'pictures', 'piece', 'pill', 'pink', 'pitches', 'pity', 'place', 'places', 'plagarized', 'plain', 'plan', 'planning', 'plastic', 'plate', 'platelets', 'platform', 'play', 'playgrounds', 'playing', 'please', 'pleasure', 'plenty', 'plural', 'plus', 'pocket', 'point', 'points', 'pointy', 'poker', 'polka', 'pool', 'poor', 'pop', 'pops', 'popular', 'population', 'popup', 'por', 'positions', 'positive', 'possessions', 'possible', 'post', 'postage', 'posted', 'posting', 'posts', 'potential', 'pounds', 'poverty', 'power', 'powered', 'powerful', 'practically', 'practice', 'practices', 'prague', 'pray', 'precise', 'prefer', 'pregnancies', 'prepared', 'preparing', 'present', 'press', 'preston', 'pretend', 'pretentious', 'pretty', 'prevent', 'previously', 'price', 'priced', 'prices', 'pricey', 'pricing', 'pride', 'print', 'printed', 'private', 'prized', 'probably', 'problem', 'problems', 'process', 'product', 'professionally', 'profile', 'profound', 'program', 'programs', 'progress', 'project', 'promised', 'promote', 'prop', 'propose', 'propping', 'pros', 'protected', 'protection', 'protective', 'protector', 'protects', 'provide', 'provides', 'provoking', 'psych', 'public', 'publish', 'published', 'publisher', 'publishers', 'publishing', 'pueden', 'pull', 'purchase', 'purchased', 'purchases', 'purchasing', 'purpose', 'purse', 'purses', 'push', 'put', 'puts', 'putting', 'qid', 'quality', 'qualtity', 'que', 'question', 'questions', 'quickly', 'quieres', 'quiet', 'quietly', 'quit', 'quite', 'ra', 'rabbits', 'rags', 'rain', 'rainbow', 'rainy', 'raise', 'raiser', 'raising', 'rarely', 'rashean', 'rather', 'rave', 'reach', 'reaction', 'read', 'reader', 'readers', 'reading', 'ready', 'real', 'reality', 'realize', 'realizing', 'really', 'realy', 'reason', 'reasonably', 'rebooted', 'receive', 'received', 'recent', 'recently', 'recomiendo', 'recommendations', 'recorder', 'recumbrant', 'red', 'rediscovered', 'reduced', 'ref', 'refer', 'reference', 'referencing', 'reflects', 'refunded', 'refuse', 'refused', 'regalo', 'regarding', 'regions', 'registered', 'regular', 'regularly', 'relate', 'releases', 'relieve', 'reluctant', 'remember', 'reminded', 'reminder', 'remove', 'removed', 'renew', 'rep', 'replace', 'replacement', 'report', 'repurchase', 'repurposed', 'require', 'required', 'reread', 'research', 'researching', 'reset', 'resources', 'response', 'rest', 'restaurant', 'restraints', 'restraunts', 'restricted', 'restrictions', 'resturant', 'result', 'retirement', 'retrieval', 'return', 'returned', 'reviews', 'revolution', 'rick', 'rid', 'ride', 'riding', 'rift', 'right', 'riley', 'rip', 'rms', 'road', 'robert', 'robin', 'robyn', 'rock', 'rocking', 'rocks', 'room', 'rooms', 'rosa', 'rose', 'rosei', 'roughly', 'round', 'router', 'row', 'rude', 'ruin', 'rule', 'runners', 'running', 'runs', 'rural', 'russian', 'rvers', 'sabah', 'sadly', 'safe', 'said', 'sailboat', 'salary', 'sale', 'salem', 'samething', 'sample', 'samples', 'san', 'sandra', 'sanity', 'santa', 'sarah', 'sarcasm', 'satisfied', 'saul', 'save', 'saved', 'saver', 'saves', 'saw', 'say', 'sayin', 'saying', 'says', 'scan', 'scanned', 'scanning', 'scary', 'scenarios', 'schemes', 'schmancy', 'school', 'schoolbooks', 'schools', 'science', 'scott', 'scrabble', 'scranton', 'scratch', 'scratched', 'screen', 'screensaver', 'scroll', 'se', 'search', 'searching', 'second', 'seconds', 'section', 'see', 'seeing', 'seek', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'select', 'selection', 'selections', 'self', 'selfish', 'sell', 'sellers', 'selling', 'senator', 'send', 'sending', 'sense', 'sent', 'sentence', 'sentences', 'sept', 'september', 'series', 'seriously', 'service', 'set', 'setting', 'settling', 'several', 'sew', 'sewed', 'sexy', 'shakespeare', 'shame', 'shape', 'share', 'sharing', 'shark', 'shedding', 'shifted', 'ship', 'shipping', 'shirley', 'shirts', 'shoot', 'shop', 'shopping', 'short', 'shortly', 'shoulder', 'shouldnt', 'show', 'showed', 'showing', 'shown', 'shows', 'si', 'sick', 'side', 'sideways', 'sight', 'signed', 'similar', 'simple', 'simpler', 'simplicity', 'simply', 'since', 'single', 'singles', 'sister', 'sisters', 'sit', 'site', 'sitters', 'sitting', 'situations', 'size', 'skies', 'skill', 'skin', 'skins', 'skip', 'skips', 'skvÄ›lÃ½', 'sled', 'sleep', 'sleeping', 'sleeve', 'sleeves', 'slice', 'sliced', 'slightly', 'slim', 'slipping', 'slot', 'slowly', 'small', 'smaller', 'smart', 'smartphone', 'smell', 'smiling', 'smoooooch', 'snaps', 'snatches', 'snider', 'snippets', 'snooks', 'snow', 'snuggle', 'snuggling', 'snuggly', 'soft', 'software', 'solar', 'solo', 'solve', 'solved', 'somebody', 'somehow', 'somehowthis', 'someone', 'someones', 'something', 'sometimes', 'somewhere', 'somthing', 'son', 'song', 'songwriter', 'sons', 'soo', 'soon', 'sooner', 'sooo', 'soooo', 'sooooo', 'soothe', 'sorry', 'sort', 'sorts', 'sound', 'sounds', 'soup', 'source', 'sources', 'south', 'southern', 'space', 'spam', 'spark', 'speak', 'speaker', 'speaks', 'special', 'specialists', 'speech', 'speed', 'spell', 'spelled', 'spelling', 'spend', 'spent', 'spoiled', 'spooning', 'sports', 'spot', 'spotlight', 'spread', 'spring', 'square', 'sr', 'sr_1_2', 'stand', 'standard', 'stands', 'staples', 'starbucks', 'staring', 'start', 'started', 'starting', 'state', 'statement', 'states', 'static', 'station', 'stay', 'stays', 'steel', 'step', 'stephanie', 'stick', 'still', 'stock', 'stop', 'stopped', 'store', 'stored', 'stores', 'story', 'straight', 'stranded', 'strange', 'straps', 'stress', 'stricken', 'stroj', 'strolling', 'struck', 'struggling', 'stuck', 'student', 'students', 'stuff', 'stumble', 'stupid', 'sub', 'subject', 'sucks', 'suddenly', 'suggested', 'suggesting', 'suggestion', 'suggestions', 'suit', 'summary', 'summer', 'sumtimes', 'sun', 'sunlight', 'sunny', 'superior', 'supplies', 'support', 'supported', 'supposedly', 'sure', 'surprised', 'surrounded', 'suspect', 'suv', 'swear', 'swedish', 'sweepstakes', 'sweet', 'sweetheart', 'switch', 'sync', 'system', 'ta', 'tab', 'table', 'tackle', 'tada', 'tag', 'take', 'takes', 'tale', 'talk', 'tampoco', 'tap', 'tape', 'tapley', 'tara', 'target', 'taser', 'tastes', 'taught', 'teach', 'teachers', 'tearing', 'teased', 'tech', 'technical', 'technology', 'tecnolÃ³gico', 'tedious', 'teen', 'teenager', 'teens', 'tell', 'tells', 'tendonitis', 'terri', 'terrie', 'text', 'textbooks', 'thank', 'thanks', 'thats', 'theory', 'therefore', 'theres', 'theri', 'theyll', 'theyre', 'thia', 'thick', 'thing', 'things', 'think', 'thinking', 'third', 'thirst', 'thnx', 'tho', 'though', 'thought', 'thoughts', 'three', 'thrilled', 'thriller', 'throw', 'thumb', 'thumbnail', 'thus', 'thx', 'tick', 'tiene', 'tienes', 'til', 'till', 'timbuk', 'timbuk2', 'time', 'times', 'timing', 'tiny', 'tip', 'tips', 'tire', 'tired', 'titanic', 'title', 'titles', 'tmm_kin_title_0', 'toc', 'today', 'toddlers', 'todo', 'todos', 'together', 'toggle', 'told', 'tom', 'tomorrow', 'toms', 'tones', 'tongue', 'tons', 'took', 'tool', 'tools', 'top', 'topic', 'tornado', 'tossing', 'total', 'totally', 'touch', 'touches', 'touching', 'touchscreen', 'tough', 'tougher', 'touting', 'towards', 'towel', 'town', 'toy', 'track', 'tracker', 'trade', 'trading', 'traditional', 'traepischke', 'train', 'transfer', 'transferred', 'translation', 'transport', 'transporting', 'travel', 'traveling', 'travelling', 'treadmill', 'treasure', 'tredmill', 'tree', 'tried', 'trip', 'triple', 'trips', 'trolley', 'trouble', 'true', 'truely', 'truly', 'trust', 'try', 'trying', 'tucking', 'tuesday', 'tuf', 'turn', 'turned', 'turning', 'turns', 'tussaud', 'tutors', 'tv', 'twain', 'twice', 'twin', 'two', 'ty', 'typ0s', 'type', 'types', 'typo', 'tzw', 'ugly', 'uh', 'uk', 'ukrainian', 'um', 'un', 'una', 'unable', 'uncle', 'understand', 'understanding', 'understood', 'unfortunately', 'unit', 'united', 'unknown', 'unless', 'unlike', 'unwanted', 'update', 'updated', 'updates', 'upgrade', 'upgraded', 'upgrading', 'upon', 'ur', 'url', 'us', 'usa', 'usar', 'usb', 'use', 'used', 'useflness', 'useful', 'usefulness', 'useless', 'user', 'users', 'uses', 'using', 'usually', 'utf8', 'utter', 'valentines', 'variant', 'variety', 'vegan', 'velcro', 'venues', 'vera', 'verdad', 'versa', 'version', 'via', 'viagra', 'vice', 'vida', 'video', 'views', 'vintage', 'violet', 'vision', 'vocabulary', 'voice', 'voila', 'vouchers', 'vtech', 'wait', 'waited', 'waiters', 'waiting', 'waits', 'wake', 'wakes', 'walk', 'walker', 'walking', 'wall', 'wan', 'want', 'wanted', 'wants', 'warm', 'warmth', 'warp', 'warranty', 'wasnt', 'watch', 'watching', 'water', 'waterproof', 'waumgauhs', 'way', 'ways', 'wearing', 'web', 'website', 'websites', 'week', 'weeks', 'weight', 'weirdly', 'weirdos', 'welcome', 'well', 'went', 'werent', 'wet', 'weve', 'whatever', 'whats', 'whawha', 'wheel', 'wherever', 'whether', 'whichever', 'whim', 'whining', 'white', 'whoa', 'whole', 'whos', 'wi', 'wide', 'widen', 'wife', 'wifi', 'wikipedia', 'wild', 'willing', 'win', 'wind', 'wing', 'winter', 'wireless', 'wisdom', 'wish', 'within', 'without', 'woman', 'wonder', 'wondered', 'wonderful', 'wondering', 'wonderland', 'wont', 'woo', 'word', 'wordgame', 'words', 'work', 'working', 'works', 'world', 'worldschooling', 'worried', 'worry', 'worse', 'worth', 'would', 'wouldnt', 'wouldve', 'wow', 'wrap', 'write', 'writer', 'writers', 'writing', 'written', 'wrong', 'wrote', 'www', 'xmas', 'xxxxxxx', 'ya', 'yall', 'yay', 'yaya', 'yea', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yet', 'yiddish', 'yippee', 'yo', 'york', 'youll', 'young', 'youre', 'youth', 'youths', 'youve', 'ypur', 'yr', 'yrs', 'yuck', 'zaffino', 'zipper', 'zippers', 'zwissler', 'Ãºltimo', 'ë­¥ë¯¸']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Print the \"Bag of words\"\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the term frequency - inverse document frequency, that measures the importance of a term \n",
    "#with respect to a document or a collection of documents\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2767)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.25208385 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Document-term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray()) #Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.25208385 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray()) #TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Try to create a pipeline for implementing Task 1, parts 1 and 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fb_sentiment.csv\")\n",
    "\n",
    "comments = df[\"FBPost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for the elimination of punctuation, stopwords and uppercase letters\n",
    "\n",
    "def Clean(data, language = \"english\"):\n",
    "    \n",
    "    data = data.str.replace('[^\\w\\s]',' ')\n",
    "    \n",
    "    stop_words_en = stopwords.words(language) \n",
    "    \n",
    "    nrows = len(data)\n",
    "    stopwords_cleaned = []\n",
    "\n",
    "    for x in range(nrows):\n",
    "        out1 = [w for w in nltk.word_tokenize(data[x].lower()) if w not in stop_words_en]\n",
    "        out2 = ' '.join(out1).strip()\n",
    "        stopwords_cleaned.append(out2)\n",
    "        \n",
    "    return pd.Series(stopwords_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for the tokenization\n",
    "\n",
    "def Tokenise(stopwords_cleaned):\n",
    "\n",
    "    tok = []\n",
    "    \n",
    "    nrows = len(stopwords_cleaned)\n",
    "\n",
    "    for x in range(nrows):\n",
    "        comment = nltk.word_tokenize(stopwords_cleaned[x])\n",
    "        tok.append(comment) \n",
    "    \n",
    "    return pd.Series(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [drug, runners, u, senator, something, murder,...\n",
       "1      [heres, single, add, kindle, read, 19th, centu...\n",
       "2      [tire, non, fiction, check, http, www, amazon,...\n",
       "3         [ghost, round, island, supposedly, nonfiction]\n",
       "4      [barnes, nobles, version, kindle, much, expens...\n",
       "                             ...                        \n",
       "995       [liked, youth, oriented, think, widen, appeal]\n",
       "996    [think, point, commercial, even, borders, clos...\n",
       "997    [kindle, 3, great, product, could, happier, mine]\n",
       "998    [develop, way, share, books, big, drawback, lo...\n",
       "999                                       [love, kindle]\n",
       "Length: 1000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the pipeline\n",
    "comments.pipe(Clean).pipe(Tokenise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B  (Classification and clustering, topic model and summarisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Perform classification and clustering and provide comments (within your Python code) on your results (commenting your code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform these tasks it is used a labeled dataset containing SMS that are labeled with \"1\" if it is a spam SMS or \"0\" viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-7cd1978b90f7>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Content\"][i] = fix_encoding(df[\"Content\"][i])\n"
     ]
    }
   ],
   "source": [
    "#import dataset and fix encoding\n",
    "\n",
    "df = pd.read_csv(\"SMSCollection.csv\", sep = \";\", encoding='utf-8')\n",
    "\n",
    "for i in range(len(df)-1):\n",
    "        \n",
    "    df[\"Content\"][i] = fix_encoding(df[\"Content\"][i])\n",
    "\n",
    "df = df[[\"Content\", \"Spam\"]]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: Spam, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Spam'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Classification\n",
    "\n",
    "In the following lines I created a classification function taking as input a corpus and the actual classification of the documents and splits the dataset into a training part and a test part (10% of the dataset), train a logistic regression model on the training set and test it on the test set. \n",
    "\n",
    "The function returns the predictions of the model on the test set and the level of accuracy.\n",
    "\n",
    "In the function it is also added the clean function implemented in the previous task to clean the data (remove punctuation, remove stopwords and get all the words lower case)\n",
    "\n",
    "Particularly in this case the function is applied on the SMSCollection dataset, a logistic regression model is trained and it is created binary classifier for recognizing Spam/not Spam messages. For the logistic regression it is use the default cutoff value = 0.5. \n",
    "\n",
    "As we can see from the output the logistic model is able to recognize almost all the Spam messages with an accuracy of 98%. The accuracy represent the proportion of istances that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that takes as input the text and the classification, split them into training and test set,\n",
    "#and return a dataframe with the actual and the predicted values and the accuracy of the model:\n",
    "\n",
    "def clean_log_classification(comments, classification):\n",
    "\n",
    "    #Clean the text using the function created in the previous task\n",
    "    comments = Clean(comments)\n",
    "    \n",
    "    # Split dataset in training and test set. The length of the test set is the 10% of the length of the entire dataset.\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(comments, classification, test_size = 0.1, random_state = 1)\n",
    "    \n",
    "    #Creation of the TF-IDF features \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "    #Creation of the logistic regression object and training of the model.\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    #Testing the model on the test set\n",
    "    X_test = vectorizer.transform(X_test_raw)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    #Create a dataframe to visualize the original value of \"spam\" of test set and the predicted one\n",
    "    predictions = pd.Series(predictions)\n",
    "    pred = (pd.DataFrame({'Test':X_test_raw, 'Value':y_test})).reset_index()\n",
    "    pred[\"predictions\"] = predictions\n",
    "    \n",
    "    return pred, accuracy_score(pred[\"Value\"], pred[\"predictions\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     index                                               Test  Value  \\\n",
       " 0     1078                               yep pretty sculpture      0   \n",
       " 1     4028                       yes princess going make moan      0   \n",
       " 2      958                            welp apparently retired      0   \n",
       " 3     4642                                             havent      0   \n",
       " 4     4674  forgot 2 ask Ã¼ smth card da present lei Ã¼ want...      0   \n",
       " ..     ...                                                ...    ...   \n",
       " 553   3529  1000 winner guaranteed caller prize final atte...      1   \n",
       " 554   5488                                             k sent      0   \n",
       " 555   5134                 sday joined training started today      0   \n",
       " 556      5  freemsg hey darling 3 week word back like fun ...      1   \n",
       " 557   1289                                   happy new year u      0   \n",
       " \n",
       "      predictions  \n",
       " 0              0  \n",
       " 1              0  \n",
       " 2              0  \n",
       " 3              0  \n",
       " 4              0  \n",
       " ..           ...  \n",
       " 553            1  \n",
       " 554            0  \n",
       " 555            0  \n",
       " 556            0  \n",
       " 557            0  \n",
       " \n",
       " [558 rows x 4 columns],\n",
       " 0.982078853046595)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Application of the function by passing as parameters the column of the message dataset.\n",
    "clean_log_classification(df[\"Content\"], df[\"Spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Clustering\n",
    "\n",
    "In the following lines, K-means clustering is applied on the SMSCollection dataset. Since clustering is an unsupervised method, I do not pass to the function the actual classification of the messages, but only their text. \n",
    "\n",
    "It is applied the clean function created in the previous task on the text of the messages to clean the data (remove punctuation, remove stopwords and get all the words lower case)\n",
    "\n",
    "The function applied to the dataset returns 2 unbalanced clusters, the first containg 5103 components and the other containing 469 components.\n",
    "\n",
    "For each clusters the first 10 components of the centroids are printed: observing the centroids printed in this part, it seems that the centorid of second cluster contains words more related to advertisment and selling activity (like, free, prize, claim, mobile) that could be related to the Spam messages.\n",
    "\n",
    "The messages with the cluster attributed by k-means clustering is then printed in a data frame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-d7f53a1866bc>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Content\"][i] = fix_encoding(df[\"Content\"][i])\n"
     ]
    }
   ],
   "source": [
    "5103#import dataset and fix encoding\n",
    "\n",
    "df = pd.read_csv(\"SMSCollection.csv\", sep = \";\", encoding='utf-8')\n",
    "\n",
    "for i in range(len(df)-1):\n",
    "        \n",
    "    df[\"Content\"][i] = fix_encoding(df[\"Content\"][i])\n",
    "\n",
    "df = df[[\"Content\", \"Spam\"]]\n",
    "df = df.dropna()\n",
    "text = df[\"Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " ok\n",
      " get\n",
      " come\n",
      " gt\n",
      " lt\n",
      " ur\n",
      " good\n",
      " go\n",
      " got\n",
      " know\n",
      "Cluster 1:\n",
      " call\n",
      " sorry\n",
      " later\n",
      " free\n",
      " prize\n",
      " please\n",
      " claim\n",
      " mobile\n",
      " urgent\n",
      " contact\n"
     ]
    }
   ],
   "source": [
    "#Clean the text using the function created in the previous task and set the number of clusters desired\n",
    "comments = Clean(text)\n",
    "num_k = 2\n",
    "\n",
    "#Create the vectorizer using TfIdf vectorizer to transform the messages\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(comments)\n",
    "    \n",
    "#Implement the k-means clustering algorithm\n",
    "model = KMeans(n_clusters=num_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    " \n",
    "    \n",
    "#Retrieve the centroids and the features\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for the two classes print the centroids \n",
    "for i in range(num_k):\n",
    "    print(\"Cluster %d:\" %i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' %terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>2nd time tried 2 contact u u 750 pound prize 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Ã¼ b going esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>pity mood suggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>guy bitching acted like interested buying some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster                                            Message\n",
       "0           0  go jurong point crazy available bugis n great ...\n",
       "1           0                            ok lar joking wif u oni\n",
       "2           0  free entry 2 wkly comp win fa cup final tkts 2...\n",
       "3           0                u dun say early hor u c already say\n",
       "4           0             nah think goes usf lives around though\n",
       "...       ...                                                ...\n",
       "5567        0  2nd time tried 2 contact u u 750 pound prize 2...\n",
       "5568        0                        Ã¼ b going esplanade fr home\n",
       "5569        0                              pity mood suggestions\n",
       "5570        0  guy bitching acted like interested buying some...\n",
       "5571        0                                     rofl true name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe of the messages and the correspondent clusters\n",
    "df = DataFrame(list(zip(model.fit_predict(X), comments)), columns = [\"Cluster\",\"Message\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5345\n",
       "1     227\n",
       "Name: Cluster, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task 2: Perform topic model and provide comments on your results.\n",
    "\n",
    "In the following lines, topic model is performed on the SMSCollection dataset in order to analyze the text of the messages and determine clusters of similar words (topics).\n",
    "\n",
    "In this case it is used Latent Dirichlet Allocation (LDA) method with three topics.\n",
    "Also here the clean function created in the previous task is applied to clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-f0a325be85e8>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Content\"][i] = fix_encoding(df[\"Content\"][i])\n"
     ]
    }
   ],
   "source": [
    "#import dataset and fix encoding\n",
    "\n",
    "df = pd.read_csv(\"SMSCollection.csv\", sep = \";\", encoding='utf-8')\n",
    "\n",
    "for i in range(len(df)-1):\n",
    "        \n",
    "    df[\"Content\"][i] = fix_encoding(df[\"Content\"][i])\n",
    "\n",
    "df = df[[\"Content\", \"Spam\"]]\n",
    "df = df.dropna()\n",
    "\n",
    "text = df[\"Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a seed\n",
    "seed = 10\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#application of the clean function created in the prevous task\n",
    "text = Clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()\n",
    "cv_features=cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn.decomposition LDA with 3 topics\n",
    "TOTAL_TOPICS=3\n",
    "lda_model=LatentDirichletAllocation(n_components=TOTAL_TOPICS,max_iter=500,max_doc_update_iter=50,learning_method='online',batch_size=1740,learning_offset=50.,random_state=42,n_jobs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics=lda_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the shape of the document\n",
    "document_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=np.array(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-70dc9d4a8967>:8: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth',-1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>get, know, go, good, day, got, ok, come, love, time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>call, free, ur, txt, stop, reply, text, claim, mobile, www</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>gt, lt, call, ok, free, mins, mobile, holiday, box, latest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Term per Topic\n",
       "Topic1  get, know, go, good, day, got, ok, come, love, time       \n",
       "Topic2  call, free, ur, txt, stop, reply, text, claim, mobile, www\n",
       "Topic3  gt, lt, call, ok, free, mins, mobile, holiday, box, latest"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract 10 terms for each of the 3 topics\n",
    "\n",
    "topic_terms=lda_model.components_\n",
    "top_terms=10 # number of 'top terms'\n",
    "topic_key_terms_idxs=np.argsort(-np.absolute(topic_terms), axis=1)[:,:top_terms]\n",
    "topic_keyterms=vocabulary[topic_key_terms_idxs]\n",
    "topics=[', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "topics_df=pd.DataFrame(topics,columns=['Term per Topic'], index=['Topic'+str(t) for t in range(1,TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the words contained in the topics, it seems that the first one refers to messages sent by  users that are  acquaintances or friends, because there are words like love, time, ok etc. that generally are sent from people that know each other. So these kind of messages could be related to real users and could be classified as not Spam.\n",
    "\n",
    "The other two topics seem to be more related to commercial topics, we can find for example \"www\" that represents the link to a website, that is generally sent by company to promote its website, or words like free, latest, stop, mobile, that are more related to the commercial and advertisement activities. These could represent Spam SMS with commericial purposes not appreciated by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Perform summarisation and provide comments (within your Python code) on your results.\n",
    "\n",
    "The module summarization.summarizer provides a function for summarizing text. \n",
    "It is based on text sentences using a variation of the TextRank algorithm. \n",
    "The output consists in the most representative sentences of the text. \n",
    "The input is a string and a word_count must be provided to the function to determine how many words the output will contain.\n",
    "\n",
    "In this example it is provided a text about World War 2 that is correctly summarized by the function with a maximum of 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open a text related to the World War 2 \n",
    "f = open('ww2.txt', 'r')\n",
    "content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan, along with other countries later on.\n",
      "The war in Europe concluded with the liberation of German-occupied territories, and the invasion of Germany by the Western Allies and the Soviet Union, culminating in the fall of Berlin to Soviet troops, the suicide of Adolf Hitler and the German unconditional surrender on 8 May 1945.\n",
      "Meanwhile, the victorious Allies of World War I, such as France, Belgium, Italy, Romania, and Greece, gained territory, and new nation-states were created out of the collapse of Austria-Hungary and the Ottoman and Russian Empires.\n"
     ]
    }
   ],
   "source": [
    "#Application of the summarization function from Gensim\n",
    "print(summarize(content, word_count=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
